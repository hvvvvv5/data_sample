{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fe51c9",
   "metadata": {},
   "source": [
    "# I NUMPY AND PANDAS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf1eff",
   "metadata": {},
   "source": [
    "## 1. SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b774ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thư viện\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94666f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([25,26,np.NaN])\n",
    "# no index: start 0 ->..\n",
    "s2 = pd.Series([25,26,np.NaN], index = [1,2,3])\n",
    "# index: following to declare\n",
    "# dtype: float64, \n",
    "print(s1)\n",
    "print('Số phần tử (khác null):',s1.count())\n",
    "#print(\"Sum:\", s2.sum())\n",
    "#print(\"Mean:\", s2.mean())\n",
    "#print(\"Median:\",s2.median()) \n",
    "print(\"Mode:\\n\",s2.mode() )\n",
    "#print(\"Std:\", s2.std()) \n",
    "print(s2.values)\n",
    "print(\"Min:\",s2.min())\n",
    "print(\"Max:\",s2.max())\n",
    "print(\"Abs:\",s2.abs())\n",
    "print(\"Prod:\",s2.prod()) \n",
    "print(\"Cumsum:\\n\",s2.cumsum())\n",
    "print(\"Cumprod: \\n\",s2.cumprod())\n",
    "print(\"Describe percentile: \\n\",s2.describe())\n",
    "print(\"Mapping math: \\n\",s2.map(lambda x:x*x))\n",
    "# 1. extract elements\n",
    "s2[0] \n",
    "s2[1:2] \n",
    "# 2. update elements\n",
    "s2[0] = 'e'\n",
    "# 3. drop elements\n",
    "s2 = s2.drop(1)\n",
    "s2_drop = s2.drop([1,2])\n",
    "# 4. index\n",
    "s2.axes\n",
    "# 5. type of elements\n",
    "s2.dtype\n",
    "# 6. check empty\n",
    "s2.empty\n",
    "# 7. dimension of series\n",
    "s2.ndim\n",
    "# 8. Size of series\n",
    "s2.size\n",
    "# 9. shape of series\n",
    "s2.shape\n",
    "# 10. values of series \n",
    "s2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941448fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0146a4",
   "metadata": {},
   "source": [
    "## 2. DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f101f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.Tạo dataframe từ list\n",
    "\n",
    "df1=pd.DataFrame(['1','2','3'])\n",
    "df1\n",
    "\n",
    "df1=pd.DataFrame([['An','19'],['Hoa',25]])\n",
    "df1\n",
    "\n",
    "df1=pd.DataFrame([['An','19'],['Hoa',25]],columns=['Ten','Tuoi'])\n",
    "df1\n",
    "\n",
    "# B.Tạo dataframe từ list dictionary\n",
    "\n",
    "df2 = pd.DataFrame([{'Ten':'An','Tuoi':19},{'Ten':'Hoa','Tuoi':25}])\n",
    "print(df2)\n",
    "\n",
    "df2 = pd.DataFrame({'Ten':['An','Hoa'],'Tuoi':[19,25]})\n",
    "print(df2)\n",
    "\n",
    "df2 = pd.DataFrame({'Ten':['An','Hoa','Trung'],'Tuoi':[19,25,30]}) \n",
    "print(df2)\n",
    "\n",
    "df2 = pd.DataFrame([{'Ten':'An','Tuoi':19},{'Ten':'Hoa','Tuoi':25},{'Ten': 'Trung'}])\n",
    "print(df2)\n",
    "\n",
    "# C. Tạo data frame từ list dictionary \n",
    "data = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}]\n",
    "# 2 cột, tên cột là tên key của dictionary\n",
    "df1 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b'])\n",
    "\n",
    "# D. 2 cột, tên một cột là tên key của dictionary, tên một cột khác\n",
    "df2 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b1'])\n",
    "print(df1)\n",
    "print(df2)\n",
    "\n",
    "# E.Tạo dataframe từ dictionary series\n",
    "\n",
    "data={'Ten':pd.Series(['Hoa','An','Trung']), 'Tuoi':pd.Series([19,25,40])}\n",
    "df3 = pd.DataFrame(data)\n",
    "\n",
    "print(df3)\n",
    "print(df3.dtypes)\n",
    "\n",
    "# F.Create a DataFrame from Dict of Series\n",
    "d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),\n",
    "      'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']),\n",
    "      'three' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),\n",
    "    }\n",
    "\n",
    "df5 = pd.DataFrame(d, columns = ['one', 'two', 'three'])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af87e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Ten':pd.Series(['Hoa','An','Trung']), 'Tuoi':pd.Series([19,25,40])}\n",
    "df3 = pd.DataFrame(data)\n",
    "\n",
    "print(df3)\n",
    "print(df3.dtypes)\n",
    "type(df3) # type  df3 : series or dataframe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------- columns ----------------------------------------\n",
    "\n",
    "# 1. Rename columns\n",
    "df3.columns=['Tên', 'Tuổi']\n",
    "df3\n",
    "---- other ways\n",
    "df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n",
    "# Or rename the existing DataFrame (rather than creating a copy) \n",
    "df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "\n",
    "# 2. Get data by columns \n",
    "df3['ten']\n",
    "\n",
    "# 3. Add more columns\n",
    "\n",
    "df3['gender'] = ....\n",
    "\n",
    "# 4. del columns\n",
    "del df3['gender']\n",
    "df3\n",
    "\n",
    "--- pop\n",
    "c = df3.pop('gender') --- pop out column 'four'\n",
    "df3.pop('four')\n",
    "\n",
    "-- update columns\n",
    "df3['gender'] = c\n",
    "\n",
    "# 5. get name of columns\n",
    "\n",
    "df3.columns\n",
    "\n",
    "\n",
    "# ---------------------------------------- row ----------------------------------------\n",
    "\n",
    "# 1. get data by row\n",
    "\n",
    "# A. index with str:\n",
    "\n",
    "df3.loc[1] # also use number\n",
    "# ---- if index a,b\n",
    "df3.loc['a'] \n",
    "df4.loc['b','four']#--- specific data [row, col]\n",
    "df3.[['a','b']] \n",
    "\n",
    "# B. index with number:\n",
    "\n",
    "df3.iloc[1] #-- only for number\n",
    "df4.iloc[1,3] #--- specific data [row, col]\n",
    "\n",
    "# C. specific data and update data:\n",
    "df4.loc['c', 'four'] =29 # row, columns\n",
    "\n",
    "# 2. get data with conditions\n",
    "\n",
    "df4.loc[df4['four']>20] # all values with col 'four' >20\n",
    "df4[df4['four']>20]# all values with col 'four' >20\n",
    "df4.loc[df4['four']>20, 'three'] #select col 'three' all values with col 'four' >20 \n",
    "\n",
    "# 3. Reset index and set index\n",
    "df2 = df2.reset_index()\n",
    "df2.reset_index(inplace=True)\n",
    "df2 = df2.set_index('Ten')\n",
    "df2.set_index(['Ten','Tuoi'], inplace=True)\n",
    "\n",
    "# 4. append data with data the same columns\n",
    "df2 = df2.append(df2b, ignore_index=True) # index will be continue to append data ignore index of data append\n",
    "\n",
    "# 5. drop data among row:\n",
    "df2 = df2.drop([3,4]) # index\n",
    "#df2.drop([3,4], inplace=True)\n",
    "df2.drop(df2.index[[3,4]])\n",
    "\n",
    "# 6. sort_index & and sort_value:\n",
    "\n",
    "df2.sort_index(ascending=False)\n",
    "df2.sort_values(by='Ten', ascending=False)\n",
    "\n",
    "# 7. Ranking:\n",
    "\n",
    "df2.rank(ascending=False) # default true\n",
    "\n",
    "# 8. infor():\n",
    "df4.info()\n",
    "\n",
    "\n",
    "# 9. any/ all check element true false:\n",
    "\n",
    "df4.any() # 0: FALSE , other: TRUE if one of these are 1,2 dif 0 then TRUE else FALSE\n",
    "df4.all() # 0: FALSE , other: TRUE if one of both are 1,2 dif 0 then TRUE else TRUE\n",
    "\n",
    "# 10. Math calculate:\n",
    "df4.sum(axis=1) # by row\n",
    "df4.sum(axis=0) # by col\n",
    "\n",
    "# 11. Describe:\n",
    "\n",
    "df4.describe() # just show has value \n",
    "df4.describe(include='all') # show all include nan\n",
    "\n",
    "\n",
    "# 12. dataframe in list:\n",
    "\n",
    "lst = {'Ten':['Tom', 'Jenny', 'Jack'], 'Tuoi': [15, 21, 18], 'HeSo': [1.5, 2.0, 1.8], 'Phai':['Nam','Nu','Nam']}\n",
    "df2 = pd.DataFrame(lst)\n",
    "df2\n",
    "lst.append(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a628d06",
   "metadata": {},
   "source": [
    "## 3. READ FILE AND VISUALIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fde1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. file csv\n",
    "\n",
    "df = pd.read_csv('Data/daily_weather.csv', nrows=5, index_col=0)\n",
    "# ====== note ======\n",
    "\n",
    "#index_col=0: use first col for index\n",
    "# sep =\",\" : phan cach\n",
    "# nrows = ..: so luong cot\n",
    "# header = None : khong co header\n",
    "# use_cols = [] : muon lay cot nao\n",
    "# encoding = 'utf-8' : co dau\n",
    "\n",
    "# export\n",
    "df2.to_csv('data/my_data.csv', header=None, index=False)\n",
    "\n",
    "# ====== note ======\n",
    "\n",
    "#index = False : no index\n",
    "# sep =\",\" : phan cach\n",
    "# na_rep ='': thay the null bang ''\n",
    "# header = None : khong co header\n",
    "# use_cols = [] : muon lay cot nao\n",
    "# encoding = 'utf-8' : co dau\n",
    "# mode = 'w': ghi de ,...\n",
    "\n",
    "\n",
    "# 2. file excel\n",
    "pip install xlrd\n",
    "\n",
    "df3 = pd.read_excel('data/danh_sach_nhan_vien.xlsx', sheet_name='Sheet2')\n",
    "\n",
    "# ====== note ======\n",
    "\n",
    "#index_col=0: use first col for index\n",
    "# sep =\",\" : phan cach\n",
    "# nrows = ..: so luong cot\n",
    "# header = None : khong co header\n",
    "# use_cols = [] : muon lay cot nao\n",
    "# encoding = 'utf-8' : co dau\n",
    "# sheet_name: sheet nao\n",
    "\n",
    "# other way \n",
    "\n",
    "f = pd.ExcelFile('data/danh_sach_nhan_vien.xlsx')\n",
    "\n",
    "df3b = pd.read_excel(f, 'Sheet2')\n",
    "df3b\n",
    "\n",
    "#-- export \n",
    "\n",
    "df3.to_excel('data/danh_sach_nhan_vien2.xlsx', 'nhanvien')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed117b",
   "metadata": {},
   "source": [
    "### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36df592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split - > results list in dataframe, series\n",
    "\n",
    "    kq = df1['diachi'].str.split(' ')\n",
    "\n",
    "# 2. use contain\n",
    "\n",
    "    df1['diachi'].str.contains('Nguyen Van Cu')\n",
    "    # results will be TRUE if row contain text else FALSE\n",
    "    df1[df1['diachi'].str.contains('Nguyen Van Cu')]\n",
    "    # results get row contain text above\n",
    "\n",
    "# 3. use replace \n",
    "\n",
    "    # a. cach 1\n",
    "    df1['diachi'] = df1['diachi'].str.replace('Nguyen', 'Nguyễn')\n",
    "    # note : only replace col\n",
    "\n",
    "    # b. cach 2\n",
    "    df2 = df2.replace(2, 22)\n",
    "    # note : replace all\n",
    "    # c. cach 3\n",
    "    df2.replace(2, 22, inplace=True)\n",
    "    # note : replace all\n",
    "\n",
    "    # more example \n",
    "    df3['phai'] = df3['phai'].str.replace('Nam','Male')\n",
    "    df3['phai'].replace('Nam','Male', inplace=True)\n",
    "\n",
    "# 4. extract\n",
    "\n",
    "    kq = df2['diachi'].str.extract('([0-9]{1,})', expand=True)\n",
    "    # expand default True : If True, return DataFrame with one column per capture group. \n",
    "    # ---> If False, return a Series/Index if there is one capture group or DataFrame if there are multiple capture groups.\n",
    "\n",
    "# 5. fillna\n",
    "\n",
    "    df2['two'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    df2['one'].fillna(method='backfill', inplace=True)\n",
    "    # example\n",
    "    round(df2['three'].mean(),2) # --> results 8.67\n",
    "    df2['three'].fillna(round(df2['three'].mean(),2))\n",
    "    df2['one'].fillna(round(df2['one'].mean(),2))\n",
    "    # 2 col with dif value replace\n",
    "    df2.fillna(value={'one': round(df2['one'].mean(),2), 'three': round(df2['three'].mean(),2)})\n",
    "    \n",
    "# 6 .drop \n",
    "\n",
    "    df2.dropna(axis=0) # 0 : drop na col\n",
    "    df2.dropna(axis=1) # 1 : drop na row\n",
    "    \n",
    "# 7. duplicated\n",
    "    \n",
    "    # check dup\n",
    "    df4.duplicated() \n",
    "    df4.drop_duplicates(keep='last') # drop dup keep last or first\n",
    "    df4.drop_duplicates(subset=['number','name']) # drop if 2 cols have row the same value\n",
    "\n",
    "# 8. interpolate()\n",
    "    df2.interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de7f43",
   "metadata": {},
   "source": [
    "## CONNECT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. pd.concat\n",
    "\n",
    "    df_new2 = pd.concat([df3, df4], axis=1, join='inner')\n",
    "    df_new2\n",
    "    # ---- note\n",
    "    # axis=1: row\n",
    "    # join='inner' - > defalt : outer\n",
    "    # sort = True - > a->b, FalseL no sort\n",
    "    df_new2b = pd.concat([df3, df4], axis=1, join='outer', sort=True)\n",
    "    df_new2b\n",
    "\n",
    "# 2. pd.append\n",
    "\n",
    "    df_new1 = df1.append(df2, sort=False, ignore_index=True)\n",
    "    df_new1\n",
    "    # ignore_index=True : del index\n",
    "\n",
    "# 3. pd.merge\n",
    "\n",
    "    # Ket nham dong do ket dua theo index\n",
    "    df_new2 = pd.concat([df3, df4], axis=1, join='inner')\n",
    "    df_new2\n",
    "    \n",
    "    # > connect data by merge corretly\n",
    "    df_new2 = pd.merge(df3, df4, left_on='ma', right_on='maso')\n",
    "    df_new2\n",
    "    df_new2b = pd.merge(df3, df4, left_index=True, right_index=True)\n",
    "    df_new2b\n",
    "\n",
    "    # how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’\n",
    "    # left: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "\n",
    "    # right: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "\n",
    "    # outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "\n",
    "    # inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n",
    "\n",
    "    # cross: creates the cartesian product from both frames, preserves the order of the left keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c7d7e",
   "metadata": {},
   "source": [
    "## CONVERT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e436bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_time'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a1828",
   "metadata": {},
   "source": [
    "## CONNECT DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python and sql\n",
    "df = pd.read_sql('%script sql', conn)\n",
    "---write many files excel \n",
    "writer = pd.ExcelWriter('..../filename.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='...', index=False)\n",
    "writer.save()\n",
    "df.to_excel('link/filename.xlsx') --export excel use power query\n",
    "df.to_sql('tablename', con=connection, if_exists='append')\n",
    "if_exists{‘fail’, ‘replace’, ‘append’}, default ‘fail’\n",
    "How to behave if the table already exists.\n",
    "fail: Raise a ValueError.\n",
    "\n",
    "replace: Drop the table before inserting new values.\n",
    "\n",
    "append: Insert new values to the existing table.\n",
    "import datetime as dt\n",
    "print('done table at: {}'.format(dt.datetime.now()))\n",
    "---write many files excel\n",
    "from pathlib import Path  \n",
    "filepath = Path('folder/subfolder/out.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)\n",
    "\n",
    "-- zip\n",
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='out.csv')  \n",
    "df.to_csv('out.zip', index=False,\n",
    "          compression=compression_opts)  \n",
    "# c1:\n",
    "result = %sql select * from ...\n",
    "df = pd.DataFrame(result)\n",
    "# c2:\n",
    "df= pd.read_sql('select * from ...', conn)\n",
    "\n",
    "Note query sql : \"\"\" \"\"\",''' ''', ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
